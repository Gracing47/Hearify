# ğŸ§  Hearify App Architecture v4.0 â€” Third-Party Developer Guide

> **Version:** 4.0  
> **Last Updated:** 2026-01-05  
> **Philosophy:** Spatial thoughts, temporal conversation, archival memory, connected insights.  
> **Target Audience:** External developers, contributors, and integration partners.

---

## ğŸ“‹ Table of Contents

1. [Executive Summary](#-executive-summary)
2. [Technology Stack](#ï¸-technology-stack)
3. [Core Pillars](#ï¸-core-pillars)
4. [Data Architecture](#-data-architecture)
5. [State Management](#-state-management)
6. [AI & Intelligence Layer](#-ai--intelligence-layer)
7. [Navigation System](#-navigation-system)
8. [File Structure](#-file-structure)
9. [Design Language](#-design-language)
10. [Getting Started](#-getting-started)
11. [Enhancement Ideas](#-enhancement-ideas)

---

## ğŸ“Œ Executive Summary

**Hearify** is a React Native (Expo) application that transforms thought capture into a spatial, AI-enhanced experience. Users speak or type their thoughts, which are:

1. **Classified** into Facts, Feelings, or Goals
2. **Vectorized** for semantic similarity search
3. **Visualized** as a 3D neural network
4. **Connected** automatically via AI-powered edge detection

The app uses a **Trinity Interface**: three screens representing different cognitive modes (Orbit â†’ Input, Horizon â†’ Visualization, Chronicle â†’ Archive).

---

## ğŸ› ï¸ Technology Stack

| Category | Technology | Version | Purpose |
|----------|------------|---------|---------|
| **Framework** | React Native (Expo) | 54.x | Cross-platform mobile |
| **Language** | TypeScript | 5.9.x | Type safety |
| **Rendering** | @shopify/react-native-skia | 2.2.12 | GPU-accelerated canvas, shaders |
| **Animation** | react-native-reanimated | 4.1.x | 60fps worklet animations |
| **Gestures** | react-native-gesture-handler | 2.28.x | Simultaneous pan/pinch/tap |
| **Database** | @op-engineering/op-sqlite | 15.x | Native SQLite + vector extensions |
| **State** | Zustand | 5.x | Lightweight global state |
| **AI (Chat)** | OpenAI GPT-4o-mini | - | Fast conversational responses |
| **AI (TTS)** | OpenAI TTS-1-HD | - | High-quality text-to-speech |
| **AI (Reasoning)** | DeepSeek R1 | - | Deep insight generation |
| **Haptics** | expo-haptics | 15.x | Tactile feedback |
| **Blur Effects** | expo-blur | 15.x | iOS glassmorphism |

### Key Dependencies (package.json)

```json
{
  "dependencies": {
    "@shopify/react-native-skia": "2.2.12",
    "react-native-reanimated": "~4.1.1",
    "react-native-gesture-handler": "~2.28.0",
    "@op-engineering/op-sqlite": "^15.1.14",
    "zustand": "^5.0.2",
    "expo": "~54.0.30"
  }
}
```

---

## ğŸ—ï¸ Core Pillars

The application rests on three interconnected screens, each representing a cognitive mode:

### 1. ğŸŒŒ Horizon (Spatial Cognition)
**Files:** `NeuralCanvas.tsx`, `HorizonScreen.tsx`

The **Horizon** is a 3D visualization of the user's mind as a neural network.

| Feature | Description |
|---------|-------------|
| **Nodes** | Each thought is a node with shape based on type |
| **Edges** | Semantic connections between related thoughts |
| **Physics** | "Raptor Physics" via Reanimated worklets |
| **LOD System** | Labels appear at higher zoom levels |
| **Neural Lenses** | Filter by type (EXPLORE/LEARN/STRATEGY/REFLECT) |

**Semantic Shapes:**
- **Hexagon** â†’ Facts (Cyan `#00F0FF`)
- **Diamond** â†’ Goals (Gold `#FFD700`)
- **Circle** â†’ Feelings (Pink `#FF0055`)

**Gestures:**
- Pan â†’ Navigate the canvas
- Pinch â†’ Zoom in/out (precision-mapped)
- Tap Node â†’ Open Thought Action Modal

### 2. âš›ï¸ Orbit (Temporal Cognition)
**Files:** `OrbitScreen.tsx`, `NeuralOrb.tsx`

The **Orbit** is the conversational interface â€” the "Now".

| Feature | Description |
|---------|-------------|
| **Thinking Orb** | Skia shader representing AI state |
| **Voice Input** | Real-time speech-to-text |
| **Text Input** | Manual thought entry |
| **Ghost Suggestions** | ACE-powered related thoughts |
| **Interruptible AI** | Stop AI mid-response |

### 3. ğŸ“œ Chronicle (Archival Cognition)
**Files:** `MemoryScreen.tsx`, `ThreadScreen.tsx`

The **Chronicle** is the memory archive â€” a timeline of all captured thoughts.

| Feature | Description |
|---------|-------------|
| **Timeline View** | SectionList grouped by date |
| **Insight Header** | Weekly summary with sparklines |
| **Shape Icons** | Same visual language as Horizon |
| **Time-Travel** | Tap card â†’ Navigate to Horizon |

---

## ğŸ’¾ Data Architecture

### Database Schema (`db/schema.ts`)

**Primary Table: `snippets`**
```sql
CREATE TABLE snippets (
  id INTEGER PRIMARY KEY AUTOINCREMENT,
  content TEXT NOT NULL,
  type TEXT CHECK(type IN ('fact', 'feeling', 'goal')),
  sentiment TEXT CHECK(sentiment IN ('analytical', 'positive', 'creative', 'neutral')),
  topic TEXT DEFAULT 'misc',
  timestamp INTEGER NOT NULL,
  x REAL DEFAULT 0,           -- 3D position
  y REAL DEFAULT 0,
  z REAL DEFAULT 0,
  importance REAL DEFAULT 1.0,
  connection_count INTEGER DEFAULT 0,
  last_accessed INTEGER,
  cluster_id INTEGER,
  cluster_label TEXT,
  reasoning TEXT,             -- AI reasoning chain
  utility_data TEXT           -- JSON for personas (flashcards, etc.)
);
```

**Vector Tables (sqlite-vec):**
```sql
-- Fast search (384-dim for real-time)
CREATE VIRTUAL TABLE vec_snippets_fast USING vec0(
  id INTEGER PRIMARY KEY,
  embedding float[384]
);

-- Rich search (1536-dim for deep context)
CREATE VIRTUAL TABLE vec_snippets USING vec0(
  id INTEGER PRIMARY KEY,
  embedding float[1536]
);
```

**Supporting Tables:**
- `semantic_edges` â€” Pre-computed thought connections
- `cluster_centroids` â€” Cluster metadata for visualization
- `daily_deltas` â€” AI-generated daily summaries
- `feedback_signals` â€” User accept/reject for trust learning
- `external_resources` â€” MCP resource ingestion (planned)

### TypeScript Interface

```typescript
interface Snippet {
  id: number;
  cluster_id: number;
  content: string;
  type: 'fact' | 'feeling' | 'goal';
  sentiment: 'analytical' | 'positive' | 'creative' | 'neutral';
  topic: string;
  timestamp: number;
  embedding?: Float32Array;
  x: number;
  y: number;
  z: number;
  importance: number;
  connection_count: number;
  last_accessed: number | null;
  reasoning?: string;
  utility_data?: string;
}
```

---

## ğŸ”„ State Management

All global state uses **Zustand** stores:

### 1. CognitiveTempoController (CTC)
**File:** `store/CognitiveTempoController.ts`

The "governor" of all motion and visual intensity:

```typescript
type CognitiveMode = 'IDLE' | 'AWARENESS' | 'INTENT' | 'REFLECTION';

interface CTCState {
  mode: CognitiveMode;
  limits: CTCLimits;  // Motion budget, camera permissions
  touch(): void;       // Reset stillness timer
  enterReflection(): void;  // Modal opened
  exitReflection(): void;   // Modal closed
}
```

**State Transitions:**
```
IDLE â”€â”€(touch canvas)â”€â”€> AWARENESS
AWARENESS â”€â”€(gesture start)â”€â”€> INTENT
INTENT â”€â”€(gesture end + 180ms)â”€â”€> AWARENESS
AWARENESS â”€â”€(open modal)â”€â”€> REFLECTION
REFLECTION â”€â”€(close modal)â”€â”€> AWARENESS
AWARENESS â”€â”€(10s stillness)â”€â”€> IDLE
```

### 2. ContextStore
**File:** `store/contextStore.ts`

Navigation and focus state:

```typescript
interface ContextState {
  activeScreen: 'horizon' | 'orbit' | 'chronicle';
  focusNodeId: number | null;
  navigateToNode(id: number): void;
  setActiveScreen(screen: string): void;
}
```

### 3. LensStore
**File:** `store/lensStore.ts`

Neural lens filtering mode:

```typescript
type LensMode = 'EXPLORE' | 'LEARN' | 'STRATEGY' | 'REFLECT';

interface LensState {
  mode: LensMode;
  setMode(mode: LensMode): void;
}
```

### 4. PredictionStore
**File:** `store/predictionStore.ts`

ACE predictions and feedback:

```typescript
interface PredictionState {
  predictions: Prediction[];
  tier: 'PREMIUM' | 'STANDARD' | 'ECO';
  setPredictions(p: Prediction[]): void;
  clearPredictions(): void;
}
```

---

## ğŸ¤– AI & Intelligence Layer

### Service Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  AI Service Layer                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  openai-chat.ts     â”‚ GPT-4o-mini responses             â”‚
â”‚  openai-tts.ts      â”‚ TTS-1-HD speech synthesis         â”‚
â”‚  deepseek.ts        â”‚ DeepSeek R1 reasoning             â”‚
â”‚  groq.ts            â”‚ Fast fallback (Llama)             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  SatelliteEngine.ts â”‚ Main AI response orchestration    â”‚
â”‚  SatelliteInsertEngine.ts â”‚ Snippet extraction from chatâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  AmbientConnectionEngine.ts â”‚ Proactive suggestions     â”‚
â”‚  SemanticDedupService.ts    â”‚ Duplicate detection       â”‚
â”‚  ThreadService.ts           â”‚ Hub-and-spoke context     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### Ambient Connection Engine (ACE)

The ACE watches user context and surfaces relevant connections:

```typescript
class AmbientConnectionEngine {
  // Debounced search with tier-aware timing
  debouncedFind(input: string, callback): void;
  
  // Record user feedback for learning
  recordFeedback(nodeId: number, action: 'ACCEPT' | 'REJECT'): void;
  
  // Performance tiers
  setTier(tier: 'PREMIUM' | 'STANDARD' | 'ECO'): void;
}
```

**Debounce Timers:**
- PREMIUM: 300ms
- STANDARD: 500ms
- ECO: 3000ms

### Thread Context Engine

For deep-diving into a thought's connections:

```typescript
interface ThreadContext {
  focus: Snippet;              // Center node
  upstream: {                  // Past/Context
    nodes: Snippet[];
    relation: 'CAUSAL' | 'TEMPORAL';
  };
  downstream: {                // Future/Implications
    nodes: Snippet[];
    relation: 'IMPLICATION' | 'NEXT_STEP' | 'AI_INSIGHT';
  };
  lateral: {                   // Related topics
    nodes: Snippet[];
    similarity: number;
  };
}
```

---

## ğŸ§­ Navigation System

### Trinity Triptych (`PanoramaScreen.tsx`)

A spatial canvas with edge-swipe navigation:

```
â† HORIZON (Graph)  |  ORBIT (Chat) [START]  |  CHRONICLE (Archive) â†’
      Zone 1              Zone 2                    Zone 3
```

**Implementation:**
- 3x screen width canvas
- 40px transparent edge zones
- Spring animation with velocity prediction
- Synced with `contextStore.activeScreen`

### Thread View (`ThreadScreen.tsx`)

Modal overlay for hub-and-spoke context:

```
ThreadScreen (Modal)
â”œâ”€â”€ Header (Back + Title)
â”œâ”€â”€ ScrollView
â”‚   â”œâ”€â”€ â¬†ï¸ UPSTREAM (Context/Past)
â”‚   â”œâ”€â”€ ğŸ¯ FOCUS HUB (Center)
â”‚   â””â”€â”€ â¬‡ï¸ DOWNSTREAM (Implications)
â””â”€â”€ TrinityActionDock (AI Actions)
    â”œâ”€â”€ ğŸ’¡ Find Pattern
    â”œâ”€â”€ ğŸ¤” Challenge
    â””â”€â”€ âš¡ Action
```

---

## ğŸ“ File Structure

```
Hearify/
â”œâ”€â”€ app/                          # Expo Router entry
â”‚   â”œâ”€â”€ _layout.tsx               # Root layout + GestureHandler
â”‚   â”œâ”€â”€ onboarding.tsx            # First-time user flow
â”‚   â””â”€â”€ (tabs)/                   # Tab navigation
â”‚
â”œâ”€â”€ components/
â”‚   â”œâ”€â”€ NeuralCanvas.tsx          # ğŸ¯ Main 3D visualization (1400+ lines)
â”‚   â”œâ”€â”€ NeuralLensesHUD.tsx       # Filter mode selector
â”‚   â”œâ”€â”€ NeuralOrb.tsx             # Thinking orb shader
â”‚   â”œâ”€â”€ GhostSuggestion.tsx       # ACE prediction cards
â”‚   â”œâ”€â”€ ThoughtActionModal.tsx    # Node action menu
â”‚   â”œâ”€â”€ FlashcardModal.tsx        # Flashcard generation
â”‚   â”œâ”€â”€ ToastContainer.tsx        # Notification system
â”‚   â”œâ”€â”€ navigation/
â”‚   â”‚   â”œâ”€â”€ PanoramaScreen.tsx    # Trinity triptych
â”‚   â”‚   â””â”€â”€ MindLayout.tsx        # Screen wrapper
â”‚   â””â”€â”€ screens/
â”‚       â”œâ”€â”€ HorizonScreen.tsx     # Graph view
â”‚       â”œâ”€â”€ OrbitScreen.tsx       # Chat interface
â”‚       â”œâ”€â”€ MemoryScreen.tsx      # Chronicle timeline
â”‚       â””â”€â”€ ThreadScreen.tsx      # Context deep-dive
â”‚
â”œâ”€â”€ constants/
â”‚   â”œâ”€â”€ contracts.ts              # ğŸ¯ All UX behavioral contracts
â”‚   â”œâ”€â”€ neuralTokens.ts           # Design tokens (colors, spacing)
â”‚   â””â”€â”€ theme.ts                  # Theme constants
â”‚
â”œâ”€â”€ db/
â”‚   â”œâ”€â”€ index.ts                  # Database initialization
â”‚   â””â”€â”€ schema.ts                 # ğŸ¯ All table definitions
â”‚
â”œâ”€â”€ hooks/
â”‚   â”œâ”€â”€ useCameraFlight.ts        # Animated camera transitions
â”‚   â”œâ”€â”€ useEcoMode.ts             # Battery-aware performance
â”‚   â”œâ”€â”€ useTTS.ts                 # Text-to-speech hook
â”‚   â””â”€â”€ useVoiceCapture.ts        # Speech-to-text hook
â”‚
â”œâ”€â”€ services/
â”‚   â”œâ”€â”€ AmbientConnectionEngine.ts # ğŸ¯ Proactive AI suggestions
â”‚   â”œâ”€â”€ SatelliteEngine.ts         # AI response generation
â”‚   â”œâ”€â”€ SatelliteInsertEngine.ts   # Snippet extraction
â”‚   â”œâ”€â”€ SemanticDedupService.ts    # Duplicate detection
â”‚   â”œâ”€â”€ ThreadService.ts           # Thread context builder
â”‚   â”œâ”€â”€ DeltaService.ts            # Daily delta summaries
â”‚   â”œâ”€â”€ openai-chat.ts             # GPT-4o-mini
â”‚   â”œâ”€â”€ openai-tts.ts              # TTS-1-HD
â”‚   â”œâ”€â”€ deepseek.ts                # DeepSeek R1
â”‚   â””â”€â”€ mcp/                       # MCP integrations (planned)
â”‚
â”œâ”€â”€ store/
â”‚   â”œâ”€â”€ CognitiveTempoController.ts # ğŸ¯ Motion governor
â”‚   â”œâ”€â”€ contextStore.ts             # Navigation state
â”‚   â”œâ”€â”€ lensStore.ts                # Neural lens mode
â”‚   â”œâ”€â”€ predictionStore.ts          # ACE predictions
â”‚   â”œâ”€â”€ toastStore.ts               # Notification queue
â”‚   â””â”€â”€ conversation.ts             # Chat history
â”‚
â”œâ”€â”€ types/
â”‚   â””â”€â”€ ThreadTypes.ts             # Thread data models
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ nlp.ts                     # Tokenization, keyword extraction
â”‚   â”œâ”€â”€ vectorMath.ts              # Cosine similarity, etc.
â”‚   â”œâ”€â”€ textAnalysis.ts            # Text processing
â”‚   â”œâ”€â”€ haptics.ts                 # Haptic feedback helpers
â”‚   â””â”€â”€ shapes.ts                  # Shape path generators
â”‚
â””â”€â”€ Docs/
    â”œâ”€â”€ MASTER_EXECUTION_PLAN.md   # Development roadmap
    â”œâ”€â”€ APP_ARCHITECTURE_V3.md     # This file
    â”œâ”€â”€ DEV_HANDOUT.md             # Sprint reports
    â”œâ”€â”€ NEURAL_CANVAS_SPEC.md      # Canvas specification
    â”œâ”€â”€ CHRONICLE_V2.md            # Chronicle features
    â””â”€â”€ THREAD_VIEW.md             # Thread engine docs
```

---

## ğŸ¨ Design Language: "Organic Immersion"

### Color Palette

| Purpose | Color | Hex |
|---------|-------|-----|
| Facts | Cyan | `#00F0FF` |
| Feelings | Pink | `#FF0055` |
| Goals | Gold | `#FFD700` |
| Focus/Accent | Purple | `#818cf8` |
| Background | Deep Black | `#0a0a12` |
| Surface | Dark Gray | `rgba(255,255,255,0.05)` |

### Typography
- System fonts (San Francisco on iOS, Roboto on Android)
- Monospace for data/stats

### Visual Effects
- **Glassmorphism:** BlurView on iOS, solid dark on Android
- **Haptics:** Rich tactile feedback on all interactions
- **Semantic Shapes:** Consistent hexagon/diamond/circle across screens

### Animation Principles
- All motion gated by CTC state
- Spring physics for natural feel
- 60fps target (30fps in Eco Mode)

---

## ğŸš€ Getting Started

### Prerequisites
- Node.js 18+
- Expo CLI
- iOS Simulator or Android Emulator (or physical device)

### Installation

```bash
# Clone the repository
git clone https://github.com/Hearify-Team/Hearify.git
cd Hearify

# Install dependencies
npm install

# Copy environment template
cp .env.example .env
# Edit .env with your API keys

# Start development server
npx expo start --clear
```

### Environment Variables

```env
OPENAI_API_KEY=sk-...         # Required for chat + TTS
DEEPSEEK_API_KEY=...          # Optional for reasoning
GROQ_API_KEY=...              # Optional fallback
```

### Running on Device

```bash
# iOS Simulator
npx expo start --ios

# Android Emulator
npx expo start --android

# Physical device (scan QR code)
npx expo start
```

---

## ğŸ’¡ Enhancement Ideas

Here are areas where third-party contributions would be valuable:

### 1. Performance Optimizations
- [ ] Implement node virtualization for 1000+ thoughts
- [ ] Add WebGL fallback for older devices
- [ ] Optimize vector search with approximate nearest neighbors

### 2. AI Enhancements
- [ ] Support for local LLMs (Ollama, llama.cpp)
- [ ] Multi-language NLP support
- [ ] Improved emotion detection

### 3. Visualization
- [ ] 3D cluster visualization (WebGL)
- [ ] Timeline heatmap view
- [ ] Export to knowledge graph formats (OWL, RDF)

### 4. Integrations
- [ ] Calendar integration (relate thoughts to events)
- [ ] Notion/Obsidian export
- [ ] Apple Health mood correlation

### 5. Accessibility
- [ ] VoiceOver/TalkBack optimization
- [ ] High contrast mode
- [ ] Reduced motion mode

### 6. Data & Privacy
- [ ] End-to-end encryption
- [ ] Local-only mode (no API calls)
- [ ] Data export/import (JSON, Markdown)

---

## ğŸ“ Contact

For questions about the architecture or contribution guidelines, please open an issue on GitHub or contact the Hearify team.

---

*Built with â¤ï¸ for the thinking human.*
